{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzmEbb76EAKn"
      },
      "source": [
        "#!pip install skorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEuu73ZjAKN8"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.autograd import Variable  \n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "\n",
        "\n",
        "from skorch import NeuralNetRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"{device}\" \" is available.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sJRQbkkAQi4"
      },
      "source": [
        "def load(paths,initial='clients'):\n",
        "    \n",
        "    data = list()\n",
        "    labels = list()\n",
        "    \n",
        "    data_folder = Path(\"./Data/\")\n",
        "    \n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(len(paths))]\n",
        "    \n",
        "    total_clients={client_names[i] : [] for i in range(len(client_names))}\n",
        "    \n",
        "    # loop over the input\n",
        "    for j in range(len(paths)):\n",
        "        \n",
        "        # load the data and extract the class labels\n",
        "        target = pd.read_csv(data_folder / paths[j], usecols=[2, 5],\n",
        "                             index_col=0, header=0)\n",
        "        target=target.loc[\"20200601\":\"20200831\"][:]\n",
        "\n",
        "        \n",
        "        target=target.to_numpy()\n",
        "\n",
        "        #sc = preprocessing.MinMaxScaler()\n",
        "        #target = sc.fit_transform(target)\n",
        "        #sc = preprocessing.StandardScaler().fit(target)\n",
        "        #target = sc.transform(target)\n",
        "        \n",
        "        ##target=target.values.tolist()\n",
        "        \n",
        "        #total_clients[client_names[j]]=list(zip((Data, target)))\n",
        "        \n",
        "        total_clients[client_names[j]]=target\n",
        "        \n",
        "        #{client_names[i] : list(zip((Data, target)))}\n",
        "        \n",
        "        \n",
        "    return  total_clients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9zxSiSoAW3g"
      },
      "source": [
        "#data_paths=['building_1_.csv','building_2_.csv','building_3_.csv','building_4_.csv','building_5_.csv','building_6_.csv','building_7_.csv',\n",
        " #           'building_9_.csv','building_10_.csv','building_11_.csv','building_12_.csv','building_13_.csv','building_14_.csv','building_16_.csv',\n",
        " #           'building_17_.csv','building_18_.csv','building_19_.csv','building_20_.csv','building_21_.csv','building_22_.csv','building_23_.csv','building_24_.csv',\n",
        " #           'building_25_.csv','building_26_.csv','building_27_.csv','building_28_.csv','building_29_.csv','building_30_.csv','building_31_.csv','building_32_.csv',\n",
        " #           'building_33_.csv','building_34_.csv','building_35_.csv','building_36_.csv','building_37_.csv','building_38_.csv','building_39_.csv','building_40_.csv',\n",
        " #           'building_41_.csv','building_42_.csv','building_43_.csv','building_44_.csv','building_45_.csv','building_46_.csv','building_47_.csv','building_48_.csv',\n",
        " #           'building_49_.csv','building_50_.csv','building_51_.csv','building_53_.csv','building_54_.csv','building_56_.csv',\n",
        "  #          'building_57_.csv','building_59_.csv','building_60_.csv','building_61_.csv','building_62_.csv','building_63_.csv','building_64_.csv',\n",
        "  #          'building_65_.csv','building_66_.csv','building_67_.csv','building_68_.csv','building_69_.csv','building_70_.csv','building_71_.csv','building_72_.csv',\n",
        "  #          'building_73_.csv','building_74_.csv','building_75_.csv','building_76_.csv','building_77_.csv','building_78_.csv','building_79_.csv','building_80_.csv','building_81_.csv']\n",
        "\n",
        "data_paths=['building_1_.csv','building_2_.csv','building_3_.csv','building_4_.csv','building_5_.csv','building_6_.csv','building_7_.csv',\n",
        "            'building_9_.csv','building_10_.csv','building_11_.csv','building_12_.csv','building_13_.csv','building_14_.csv','building_16_.csv',\n",
        "            'building_17_.csv','building_18_.csv','building_19_.csv','building_20_.csv','building_21_.csv','building_22_.csv']\n",
        "\n",
        "\n",
        "#data_paths=['building_23_.csv','building_24_.csv',\n",
        " #           'building_25_.csv','building_26_.csv','building_27_.csv','building_28_.csv','building_29_.csv','building_30_.csv','building_31_.csv','building_32_.csv',\n",
        " #           'building_33_.csv','building_34_.csv','building_35_.csv','building_36_.csv','building_37_.csv','building_38_.csv','building_39_.csv','building_40_.csv',\n",
        "  #          'building_41_.csv']\n",
        "\n",
        "\n",
        "#data_paths=['building_42_.csv','building_43_.csv','building_44_.csv','building_45_.csv','building_46_.csv','building_47_.csv','building_48_.csv',\n",
        "#            'building_49_.csv','building_50_.csv','building_51_.csv','building_53_.csv','building_54_.csv','building_56_.csv',\n",
        "#            'building_57_.csv','building_59_.csv','building_60_.csv','building_61_.csv','building_62_.csv','building_63_.csv','building_64_.csv']\n",
        "\n",
        "\n",
        "#data_paths=['building_65_.csv','building_66_.csv','building_67_.csv','building_68_.csv','building_69_.csv','building_70_.csv','building_71_.csv','building_72_.csv',\n",
        "    #        'building_73_.csv','building_74_.csv','building_75_.csv','building_76_.csv','building_77_.csv','building_78_.csv','building_79_.csv','building_80_.csv','building_81_.csv']\n",
        "\n",
        "#data_paths=['building_1_.csv']\n",
        "clients=load(data_paths,initial='clients')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUaZKItMNqgE"
      },
      "source": [
        "def sliding_windows(data, seq_length):\n",
        "    x = []\n",
        "    y = []\n",
        "  \n",
        "\n",
        "    for i in range(len(data)-seq_length-1):\n",
        "        _x = data[i:(i+seq_length)]\n",
        "        _y = data[i+seq_length]\n",
        "\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return np.array(x),np.array(y)\n",
        "\n",
        "\n",
        "\n",
        "#process and batch the test data for each client\n",
        "\n",
        "client_names = ['{}_{}'.format('clients', i+1) for i in range(len(data_paths))]\n",
        "\n",
        "x=dict()\n",
        "y=dict()\n",
        "\n",
        "seq_length = 24\n",
        "\n",
        "for i in range(len(data_paths)):\n",
        "    \n",
        "    x[client_names[i]], y[client_names[i]] = sliding_windows(clients[client_names[i]], seq_length)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9qO-PHVOGS0"
      },
      "source": [
        "train_size = int(len(y[client_names[i]]) * 0.75)\n",
        "test_size = len(y[client_names[i]]) - train_size\n",
        "\n",
        "dataX = dict()\n",
        "dataY= dict()\n",
        "\n",
        "trainX= dict()\n",
        "trainY= dict()\n",
        "\n",
        "testX= dict()\n",
        "testY= dict()\n",
        "\n",
        "for i in range(len(data_paths)):\n",
        "    \n",
        "    dataX[client_names[i]] = Variable(torch.Tensor(np.array(x[client_names[i]]))).to(device)\n",
        "    dataY[client_names[i]] = Variable(torch.Tensor(np.array(y[client_names[i]]))).to(device)\n",
        "\n",
        "    trainX[client_names[i]] = Variable(torch.Tensor(np.array(x[client_names[i]][0:train_size]))).to(device)\n",
        "    trainY[client_names[i]] = Variable(torch.Tensor(np.array(y[client_names[i]][0:train_size]))).to(device)\n",
        "\n",
        "    testX[client_names[i]] = Variable(torch.Tensor(np.array(x[client_names[i]][train_size:len(x[client_names[i]])]))).to(device)\n",
        "    testY[client_names[i]] = Variable(torch.Tensor(np.array(y[client_names[i]][train_size:len(y[client_names[i]])]))).to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k6naRB1MxCH"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, l1, l2):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.num_classes = 1\n",
        "        self.num_layers = 1\n",
        "        self.input_size = 1\n",
        "        self.l1 = l1\n",
        "        self.l2 = l2\n",
        "        self.seq_length = seq_length\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=l1,\n",
        "                            num_layers=1, batch_first=True)\n",
        "        \n",
        "        self.fc1 = nn.Linear(l1, l2)\n",
        "        self.fc2 = nn.Linear(l2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = Variable(torch.zeros(\n",
        "            self.num_layers, x.size(0), self.l1)).to(device)\n",
        "        \n",
        "        c_0 = Variable(torch.zeros(\n",
        "            self.num_layers, x.size(0), self.l1)).to(device)\n",
        "        \n",
        "        # Propagate input through LSTM\n",
        "        out, _ = self.lstm(x, (h_0, c_0))\n",
        "        out = out[:, -1, :]\n",
        "        #h_out = h_out.view(-1, self.hidden_size1)\n",
        "        \n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        #out = torch.clip(out, 0)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh0tKA2cPgf8",
        "outputId": "ffe77ad3-a791-436b-c85d-a3d047942369"
      },
      "source": [
        "All_parameters=dict()\n",
        "\n",
        "All_results=dict()\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# create the sklearn model for the network\n",
        "\n",
        "model_init_batch_epoch_CV = NeuralNetRegressor(Net, verbose=0, train_split=False, lr=0.01, batch_size=24, optimizer=torch.optim.Adam, criterion=nn.MSELoss(), device=device)\n",
        "\n",
        "\n",
        "\n",
        "#epochs = np.random.randint(80, 300, 20)\n",
        "#epochs=epochs.tolist()\n",
        "epochs=[103, 148, 190, 247, 291]\n",
        "\n",
        "#hidden_size1=np.random.randint(24, 70, 20)\n",
        "#hidden_size1=hidden_size1.tolist()\n",
        "hidden_size1=[24, 32, 44, 56, 68]\n",
        "\n",
        "hidden_size2=[85, 105, 127, 148, 198]\n",
        "#second_layer_neurons=np.random.randint(80, 200, 20)\n",
        "#second_layer_neurons=second_layer_neurons.tolist()\n",
        "\n",
        "param_grid = {\n",
        "\t'max_epochs':epochs,\n",
        "\t'module__l1': hidden_size1,\n",
        "  'module__l2': hidden_size2,\n",
        "  'lr': [0.01],\n",
        "  'optimizer': [torch.optim.Adam],\n",
        "  'criterion': [nn.MSELoss],\n",
        "  'batch_size':[24]\n",
        "\t}\n",
        "\n",
        "\n",
        "grid = GridSearchCV(model_init_batch_epoch_CV, \n",
        "                    param_grid, refit=False,\n",
        "                    cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#neg_mean_absolute_percentage_error\n",
        "\n",
        "client_names = ['{}_{}'.format('clients', i+1) for i in range(len(data_paths))]\n",
        "\n",
        "\n",
        "\n",
        "for client in client_names:\n",
        "\n",
        "\n",
        "    grid_result = grid.fit(trainX[client].cpu(), trainY[client].cpu())\n",
        "    \n",
        "    \n",
        "    All_parameters[client]=grid_result.best_params_\n",
        "    \n",
        "    All_results[client]=grid_result.cv_results_\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  8.7min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 50.6min\n",
            "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed: 136.6min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiGqmOmSP7z2"
      },
      "source": [
        "All_parameters=pd.DataFrame(All_parameters)\n",
        "All_parameters.to_csv(\"output.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puyW9zD5P-pD"
      },
      "source": [
        "All_results=pd.DataFrame(All_results)\n",
        "All_results.to_csv(\"results.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}